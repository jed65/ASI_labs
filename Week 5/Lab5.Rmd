---
title: 'MA40198: Lab sheet 5'
author: "Jake Denton, Patrick Fahy and Henry Writer"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_float: yes
    code_download: yes
always_allow_html: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, highlight = TRUE)
library(tidyverse)
library(polynom)
```


## Instructions

* This  assignment should be done in groups  according to the coursework group allocation.

 * As indicated in each question, you should complete the answers by filling-out  the 'Lab5.Rmd' file.  You can remove the instructions and introductory sections if you wish.
 
 
* The  'Lab5.Rmd' file  can be downloaded by clicking in the **Code download** button at the top right of this page. If the button does not work you can download the 'Lab4.Rmd' file from the Moodle page. You should open this file in RStudio.
 

*  After  completing the answers, each group should convert the 'Lab5.Rmd' file  into an HTML file  by following the instructions in the <a href="#knit-to-html">Knit to html</a> section at the end of this file!


* Each group should submit two files:  the completed 'Lab5.Rmd' file  and the created HTML file (which should be named 'Lab5.html') to the  <a href="https://moodle.bath.ac.uk/mod/assign/view.php?id=1172556">submission point in Moodle</a>



* This is lab is a **formative assessment** and  you will receive feedback to your submitted work on an informal basis.


##  Question 1:  Generalised Binomial distribution

Consider the following independent sample $\boldsymbol{y}=(y_1,\ldots,y_n)^T$ of size $n=20$ from a generalised Binomial distribution  with known index $N=8$ and unknown parameters $\theta_1^*$ and $\theta_2^*$. 

```{r}
dat_genbinom<-read.table(url("https://people.bath.ac.uk/kai21/ASI/data/genbinom_sample.txt"),header = T)

```


The density of a generalised binomial with parameters $\theta_1\in R$ and $\theta_2 \in R$ is given by:

$$f(y|\boldsymbol{\theta})={N\choose y} \exp\left(y\theta_1+y(N-y)\,\theta_2 -\Psi(\boldsymbol{\theta})\right)\,,\qquad y\in \{0,1,2,\ldots,N\}$$
where:
\begin{equation*}
\Psi(\boldsymbol{\theta})=\log\left(\sum_{i=0}^N{N\choose i} \exp\left(i\theta_1+i(N-i)\,\theta_2 \right)\right)\,.
\end{equation*}




### Question 1.1

Use both:

* the asymptotic normality of the maximum likelihood estimators and 

* the generalised likelihood ratio test  (GLRT)

to test the following hypotheses:

$$H_0:\,\theta^*_2=0 \qquad \mbox{vs}\qquad H_a:\,\theta^*_2\neq 0$$



In both cases set an approximate significance level of $0.05$.

```{r}

# density of the generalised Binomial
# dgenbinom<-function(x,theta1,theta2){ 
#   N <- 8
#   xx       <- 0:N # support set
#   phi      <- log(sum(choose(N, xx)*exp(xx*theta1+xx*(N-xx)*theta2)))
#   res <- choose(N, x)*exp(x*theta1+x*(N-x)*theta2-phi)
#   return(res)
# }



# negloglik_fn<-function(theta = c(0,0), data = 1){
#   sum_log_dens <- function(x,theta1,theta2){
#       -sum(log(dgenbinom(x,theta1,theta2))) 
#   }
#   mapply(FUN      = sum_log_dens,
#          theta1   = theta[1],
#          theta2   = theta[2],
#          MoreArgs = list(x = data))
# }

negloglik_fn <- function(theta, data){
  N <- 8
  xx <- 0:N
  n <- length(data)
  phi <- log(sum(choose(N, xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2])))
  res <- -sum(lchoose(N, data))+n*phi-theta[1]*sum(data)-theta[2]*sum(data*(N-data))
  return (res)
}

# gradient of the loglikelihood
negloglik_grad<-function(theta,data){
  N <- 8
  n <- length(data)
  xx <- 0:N
  phi <- log(sum(choose(N, xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2])))
  first_sum <- sum(choose(N, xx)*xx*exp(xx*theta[1]+xx*(N-xx)*theta[2]-phi))
  second_sum <- sum(choose(N, xx)*xx*(N-xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2]-phi))
  wrttheta1 <- first_sum
  wrttheta2 <- second_sum
  grad <- -c(sum(data) - n*wrttheta1, sum(data*(N-data))  - n*wrttheta2)
  return (grad)
}


opt <- optim(par     = c(0,0),
             fn      = negloglik_fn,
             gr      = negloglik_grad,
             method="BFGS",
             data    = dat_genbinom$x,
             hessian=TRUE)

thetas<-opt$par
j_inv = solve(opt$hessian)

z = qnorm(0.95)

CI_theta2 <- c(thetas[2] - z*sqrt(j_inv[2,2]), thetas[2]+z*sqrt(j_inv[2,2]))

## CI contains 0 so null hypothesis isn't rejected


negloglik_fn2 <- function(theta, data){
  N <- 8
  xx <- 0:N
  n <- length(data)
  phi <- log(sum(choose(N, xx)*exp(xx*theta)))
  res <- -sum(lchoose(N, data))+n*phi-theta*sum(data)
  return (res)
}

# gradient of the loglikelihood
negloglik_grad2<-function(theta,data){
  N <- 8
  n <- length(data)
  xx <- 0:N
  phi <- log(sum(choose(N, xx)*exp(xx*theta)))
  first_sum <- sum(choose(N, xx)*xx*exp(xx*theta-phi))
  wrttheta1 <- first_sum
  grad <- -sum(data) + n*wrttheta1
  return (grad)
}


opt2 <- optim(par     = 0,
             fn      = negloglik_fn2,
             gr      = negloglik_grad2,
             method="BFGS",
             data    = dat_genbinom$x,
             hessian=TRUE)

thetas2<-opt2$par

l1 <- negloglik_fn(thetas, dat_genbinom$x)
l2 <- negloglik_fn2(thetas2, dat_genbinom$x)

2*(l2-l1) > qchisq(p=.05, df=1, lower.tail=FALSE)

## False, so null hypothesis isn't rejected

```



### Question 1.2

Use both:

* the asymptotic normality of the maximum likelihood estimators and 

* the generalised likelihood ratio test  (GLRT)

to test the following hypotheses:

$$H_0:\,3\lambda^*_1+4\lambda_2^*=60 \qquad \mbox{vs}\qquad H_a:\,3\lambda^*_1+4\lambda_2^*\neq 60$$
where  
$$
\begin{pmatrix}
\lambda_1\\
\lambda_2
\end{pmatrix}
=
\begin{pmatrix}
E_{\boldsymbol{\theta}}[Y]\\
E_{\boldsymbol{\theta}}[Y(N-Y)]
\end{pmatrix}
$$


and $Y$ is random variable following a generalised Binomial with parameters $\boldsymbol{\theta}=(\theta_1,\theta_2)^T$.

In both cases set an approximate significance level of $0.05$.



```{r}
negloglik_fn <- function(theta, data){
  N <- 8
  xx <- 0:N
  n <- length(data)
  phi <- log(sum(choose(N, xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2])))
  res <- -sum(lchoose(N, data))+n*phi-theta[1]*sum(data)-theta[2]*sum(data*(N-data))
  return (res)
}

# gradient of the loglikelihood
negloglik_grad<-function(theta,data){
  N <- 8
  n <- length(data)
  xx <- 0:N
  phi <- log(sum(choose(N, xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2])))
  first_sum <- sum(choose(N, xx)*xx*exp(xx*theta[1]+xx*(N-xx)*theta[2]-phi))
  second_sum <- sum(choose(N, xx)*xx*(N-xx)*exp(xx*theta[1]+xx*(N-xx)*theta[2]-phi))
  wrttheta1 <- first_sum
  wrttheta2 <- second_sum
  grad <- -c(sum(data) - n*wrttheta1, sum(data*(N-data))  - n*wrttheta2)
  return (grad)
}


opt <- optim(par     = c(0,0),
             fn      = negloglik_fn,
             gr      = negloglik_grad,
             method="BFGS",
             data    = dat_genbinom$x,
             hessian = TRUE)

thetas<-opt$par

j_inv = solve(opt$hessian)

CI_theta2 <- c(thetas[2] - z*sqrt(j_inv[2,2]), thetas[2]+z*sqrt(j_inv[2,2]))

findl2func <- function(theta2){
  phi <- log(sum(choose(N, xx)*exp(xx*thetas[1]+xx*(N-xx)*theta2)))
  lambda1 <- sum(choose(N, xx)*xx*exp(xx*thetas[1]+xx*(N-xx)*theta2-phi))
  lambda2 <- sum(choose(N, xx)*xx*(N-xx)*exp(xx*thetas[1]+xx*(N-xx)*theta2-phi))
  return ((3*lambda1 + 4*lambda2-60)^2)
}
theta2 <- optimize(f=findl2func, interval=c(-100,100))$minimum



# theta2 which satisfies the null hypothesis is contained within the confidence interval in the alternative hypothesis, insufficient evidence to reject the null hypothesis


negloglik_fn2 <- function(theta, data){
  N <- 8
  xx <- 0:N
  n <- length(data)
  findl2func <- function(theta2){
    phi <- log(sum(choose(N, xx)*exp(xx*theta+xx*(N-xx)*theta2)))
    lambda1 <- sum(choose(N, xx)*xx*exp(xx*theta+xx*(N-xx)*theta2-phi))
    lambda2 <- sum(choose(N, xx)*xx*(N-xx)*exp(xx*theta+xx*(N-xx)*theta2-phi))
    return ((3*lambda1 + 4*lambda2-60)^2)
  }
  theta2 <- optimize(f=findl2func, interval=c(-100,100))$minimum
  phi <- log(sum(choose(N, xx)*exp(xx*theta+xx*(N-xx)*theta2)))
  res <- -sum(lchoose(N, data))+n*phi-theta*sum(data)-theta2*sum(data*(N-data))
  return (res)
}

# gradient of the loglikelihood
negloglik_grad2<-function(theta,data){
  N <- 8
  n <- length(data)
  xx <- 0:N
  findl2func <- function(theta2){
    phi <- log(sum(choose(N, xx)*exp(xx*theta+xx*(N-xx)*theta2)))
    lambda1 <- sum(choose(N, xx)*xx*exp(xx*theta+xx*(N-xx)*theta2-phi))
    lambda2 <- sum(choose(N, xx)*xx*(N-xx)*exp(xx*theta+xx*(N-xx)*theta2-phi))
    return((3*lambda1 + 4*lambda2-60)^2)
  }
  theta2 <- optimize(f=findl2func, interval=c(-100,100))$minimum
  phi <- log(sum(choose(N, xx)*exp(xx*theta+xx*(N-xx)*theta2)))
  first_sum <- sum(choose(N, xx)*xx*exp(xx*theta+xx*(N-xx)*theta2-phi))
  wrttheta1 <- first_sum
  grad <- -sum(data) + n*wrttheta1
  return (grad)
}



opt2 <- optim(par     = 0,
             fn      = negloglik_fn2,
             gr      = negloglik_grad2,
             method="BFGS",
             data    = dat_genbinom$x)

thetas2<-opt2$par

l1 <- negloglik_fn(thetas, dat_genbinom$x)
l2 <- negloglik_fn2(thetas2, dat_genbinom$x)

2*(l2-l1) > qchisq(p=.05, df=1, lower.tail=FALSE)

## True, so there is evidence to reject the null hypothesis
```




## Question 2: Gamma distribution



Consider the following independent sample $\boldsymbol{y}=(y_1,\ldots, y_n)^T$ of size $n=20$ from Gamma distribution with unknown  shape parameter $\alpha^*>0$ and  unknown rate parameter $\beta^*>0$

```{r}
dat_gamma<-read.table(url("https://people.bath.ac.uk/kai21/ASI/data/gamma_sample.txt"),header = T)
```


The  probability density function of a Gamma distribution with shape $\alpha>0$ and rate $\beta>0$ is given by
$$f(y|\alpha,\beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\,y^{\alpha-1}e^{-\beta\,y}\,,\quad\mbox{for }\quad y>0$$

### Question 2.1

Use both:

* the asymptotic normality of the maximum likelihood estimators and 

* the generalised likelihood ratio test  (GLRT)

to test the following hypotheses:

$$H_0:\,\alpha^*=(\beta^*)^2 \qquad \mbox{vs}\qquad H_a:\alpha^*\neq(\beta^*)^2$$

In both cases set an approximate significance level of $0.05$.

```{r}
#Read in the data, 20 obs. of one variable
dat_gamma<-read.table(url("https://people.bath.ac.uk/kai21/ASI/data/gamma_sample.txt"),header = T)

#### Reparametrisation and optimisation ####
#Consider parametrisation given by lambda=(log(alpha),log(beta))
#This takes onto the unconstrained real domain, where we can use optim()

nll_lambda<-function(lambda,data){ #Compute negative log-likelihood
  logdata<-log(data)
  n<-max(dim(data))
  nll<- -n*lambda[2]*exp(lambda[1])+n*lgamma(exp(lambda[1]))-(exp(lambda[1])-1)*sum(logdata)+exp(lambda[2])*sum(data)
  return(nll)
}

nll_lambda_grad<-function(lambda,data){ #Compute gradient of neg. log-likelihood
  logdata<-log(data)
  n<-max(dim(data))
  grad<-matrix(0,nrow=2,ncol=1)
  grad[1]<- -n*lambda[2]*exp(lambda[1])+n*exp(lambda[1])*digamma(exp(lambda[1]))-exp(lambda[1])*sum(logdata)
  grad[2]<- -n*exp(lambda[1])+exp(lambda[2])*sum(data)
  return(grad)
}

#Now use optim to minimise function with respect to unconstrained parameters
fit_lambda<-optim(par     = c(0,0),
                  fn      = nll_lambda,
                  gr      = nll_lambda_grad,
                  data    = dat_gamma,
                  method = "BFGS",
                  hessian = TRUE)
#Gives us the MLE for lambda, which can be transformed back into (alpha,beta)
lambda_star<-fit_lambda$par
shape_rate_star<-exp(lambda_star)

#Can also find Hessian of lambda and using Jacobian, that of (alpha,beta)
#Note that in gamma distribution example, Hessian and fisher information are equal
hess_lambda_observed<-fit_lambda$hessian
J<-diag(exp(-c(fit_lambda$par))) #This is the Jacobian, note the symmetry and remember we are going back from lambda to theta!
hess_theta_observed<-t(J)%*%hess_lambda_observed%*%J 

#Can get inverse of this observed fisher info matrix (in terms of (alpha,beta))
inverse_fisher_theta<-solve(hess_theta_observed) #Has reasonable values

#### Hypothesis test - Confidence intervals ####
#For hypothesis test, use reparametrisation gamma=(alpha-beta^2,beta)
MLE<-c(shape_rate_star[1]-shape_rate_star[2]^2,shape_rate_star[2]) #find reparametrised MLE
J_2<-matrix(0,nrow=2,ncol=2) #Form Jacobian in this case
J_2<-J_2+diag(c(1,1))
J_2[1,2]=2*shape_rate_star[2]
hess_gamma_observed<-t(J_2)%*%hess_theta_observed%*%J_2 #Find reparametrized Fisher info
inverse_fisher_gamma<-solve(hess_gamma_observed)
#We now have all the ingredients required for hypothesis test
lower_bound=MLE[1]-qnorm(0.025,lower.tail = FALSE)*sqrt(inverse_fisher_gamma[1,1])
upper_bound=MLE[1]+qnorm(0.025,lower.tail = FALSE)*sqrt(inverse_fisher_gamma[1,1])
confidence_interval<-c(lower_bound,upper_bound)
confidence_interval
#This contains zero, insufficient evidence to reject null hypothesis

#### Hypothesis test - GLRT ####
#Use invariance property of MLE for unrestrained problem, remember GLRT uses log-likelihood (x-1)
MLE_likelihood_unrestrained<- -nll_lambda(fit_lambda$par,dat_gamma)

#Need to define negative log-likelihood and gradient in restrained case
#Note that we still have to reparametrise to use optim()
#First function is for negative log-likelihood, second is for gradient
nll_restrained<-function(lambda,data){
  logdata<-log(data)
  n<-max(dim(data))
  nll<- -(n/2)*lambda*exp(lambda)+n*lgamma(exp(lambda))-(exp(lambda)-1)*sum(logdata)+exp(lambda/2)*sum(data)
}

nll_restrained_gradient<-function(lambda,data){
  logdata<-log(data)
  n<-max(dim(data))
  grad<- -(n/2)*lambda*exp(lambda)-(n/2)*exp(lambda)+n*exp(lambda)*digamma(exp(lambda))-exp(lambda)*sum(logdata)+0.5*exp(lambda/2)*sum(data)
  return(grad)
}

#Now optimise for restrained MLE and get value of negative log-likelihood at this point
fit_lambda_1<-optim(par   = 0,
                  fn      = nll_restrained,
                  gr      = nll_restrained_gradient,
                  data    = dat_gamma,
                  method = "BFGS")
MLE_likelihood_restrained<- -nll_restrained(fit_lambda_1$par,dat_gamma)

test_statistic<-2*(MLE_likelihood_unrestrained-MLE_likelihood_restrained) #small value of t
critical_value<-qchisq(p=.95,df=1)
test_statistic>critical_value #this is false, we have insufficient evidence to reject null as before
```



## Question 3: Log-logistic distribution



Consider the following independent sample $\boldsymbol{y}=(y_1,\ldots, y_n)^T$ of size $n=50$ from a Log-logistic distribution with unknown  scale parameter $\theta_1^*>0$ and  unknown shape parameter $\theta_2^*>0$

```{r}
dat_loglogis<-read.table(url("https://people.bath.ac.uk/kai21/ASI/data/loglogistic_sample.txt"),header = T)
```

The density of a log-logistic distribution with scale parameter $\theta_1>0$ and shape parameter $\theta_2>0$ is given by:
$$\displaystyle f(y|\boldsymbol{\theta})=\frac{\frac{\theta_2}{\theta_1}\left(\frac{y}{\theta_1}\right)^{\theta_2-1}}{\left(1+\left(\frac{y}{\theta_1}\right)^{\theta_2}\right)^2}\,,\qquad y>0$$

### Question 3.1

Use both:

* the asymptotic normality of the maximum likelihood estimators and 

* the generalised likelihood ratio test  (GLRT)

to test the following hypotheses:

$$H_0:\,\theta_1^*\theta_2^*=1 \qquad \mbox{vs}\qquad H_a:\theta_1^*\theta_2^*\neq1$$

In both cases set an approximate significance level of $0.05$.

