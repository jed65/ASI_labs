---
title: 'MA40198: Lab 2: Quasi-Newton methods'
author: "Your name here"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_float: yes
    code_download: yes
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, highlight = TRUE)
```

# Instructions

-   This assignment should be done in groups according to the coursework group allocation.

-   As indicated in each question, you should complete the answers by filling-out the 'Lab2.Rmd' file. You can remove the instructions and introductory sections if you wish.

-   The 'Lab2.Rmd' file can be downloaded by clicking in the **Code download** button at the top right of this page. If the button does not work you can download the 'Lab2.Rmd' file from the Moodle page. You should open this file in RStudio.

-   After completing the answers, each group should convert the 'Lab2.Rmd' file into an HTML file by following the instructions in the <a href="#knit-to-html">Knit to html</a> section at the end of this file!

-   Each group should submit two files: the completed 'Lab2.Rmd' file and the created HTML file (which should be named 'Lab2.html') to the <a href="https://moodle.bath.ac.uk/mod/assign/view.php?id=1160443">submission point in Moodle</a>

-   This is lab is a **formative assessment** and you will receive feedback to your submitted work on an informal basis.

## Introduction

This lab is about gaining a solid understanding of quasi-Newton methods such as the BFGS algorithm to maximise loglikelihoods.

## Question

For the COVID-19 mortality [dataset](https://people.bath.ac.uk/kai21/ASI/data/COVID19_MARCH_JUNE.csv) used in [Question sheet 1](https://moodle.bath.ac.uk/pluginfile.php/2120917/mod_resource/content/1/Lab1.html), consider again the model for the mean number of deaths, that integrates the effect of sex:

$$\mbox{mortality rate per 100k}=\exp(\theta_1+\theta_2\, age+\theta_3\,\delta_{male})$$ where $\delta_{male}=1$ if male and $\delta_{male}=0$ if female.

Remember that you can load the dataset as follows:

```{r,echo=TRUE}
deaths<-read.csv("https://people.bath.ac.uk/kai21/ASI/data/COVID19_MARCH_JUNE.csv",header =TRUE)
```

-   Compute a starting value $\boldsymbol{\theta}_0$ exploiting the log-linear form of the mean model.

-   Use the BFGS algorithm to compute the maximum likelihood estimator of the unknown parameter $\boldsymbol{\theta}^T=(\theta_1,\theta_2,\theta_3)$ given the data. Did you observed any substantial difference in the iterations compared to those obtained using the Newton algorithm?

## Solution to Question {.unnumbered}

```{r Q1, eval = FALSE}
# Chunk to compute starting value
deaths<-read.csv("https://people.bath.ac.uk/kai21/ASI/data/COVID19_MARCH_JUNE.csv",header =TRUE)

deaths$rate<-deaths$deaths_COVID/(deaths$population/100000) #make variable for rate
deaths$sex_logic<-c(rep(1,17),rep(0,17)) #converting sex to a logical operator
deaths$rate_log<-log(deaths$rate) #taking the log of the death rate

multi.fit = lm(rate_log~age+sex_logic, data=deaths) #fit linear model
initial_guess = multi.fit$coefficients #extract starting theta
names(initial_guess) <- NULL #remove names, gives vector theta_0
cat('The starting value from fitting linear model is:', initial_guess, '\n')
```

```{r}
# Code chunk which designs and implements BFGS algorithm
nll <- function(theta,y,a,k,delta){ # Function to compute neg. log-likelihood
  
  lin_pred     <- theta[1]+theta[2]*a+theta[3]*delta
  
  mu           <- k*exp(lin_pred)
  
  sum_log_dens <- sum(dpois(x = y,
                            lambda = mu,
                            log  = TRUE))
  -sum_log_dens
}

nll_grad <- function(theta,y,a,k,delta){ # Function to compute gradient
  lin_pred     <- theta[1]+theta[2]*a+theta[3]*delta
  mu           <- k*exp(lin_pred)
  
  grad_1 <- sum(mu-y)
  grad_2 <- sum(a*(mu-y))
  grad_3<- sum(delta*(mu-y))
  c(grad_1,grad_2,grad_3)
}

BFGS<-function(theta_0,func,grad,B,
               control=list(maxit=1000,abstol=1e-6,c1=1e-4,c2=0.9),
               y,a,k_in,delta){
  norm_vec<- function(x) sqrt(sum(x^2)) # Compute l2 norm
  k<-1 # Iteration counter
  convergence_indicator <- 0 # 0 if successful, 1 otherwise
  d <- length(theta_0) # Dimension of our problem
  theta_current<-theta_0 # Set current value of theta
  nll_current<-func(theta_current,y,a,k_in,delta) # Evaluate l()
  grad_nll_current<-grad(theta_current,y,a,k_in,delta) # Evaluate grad l()
  theta_iterates<-matrix(0,nrow=control$maxit,ncol=d) # Matrix to hold iterates
  theta_iterates[k,]<-theta_current # Set first line in matrix to initial guess
  k_backtracking <- rep(0,control$maxit) # Keep track of how many backtracks at each iteration
  
  # Now we proceed with the main part of the algorithm
  while (norm_vec(grad_nll_current)>control$abstol){ # Stop criterion
    
    Delta <- -B %*% grad_nll_current # Computes descent direction
    
    # Backtracking section
    alpha<-1 # This is alpha_k in notes
    alpha_min<-0 
    alpha_max<-Inf
    
    # Find values for this value of step length
    theta_proposed<-theta_current+alpha*Delta
    nll_proposed<-func(theta_proposed,y,a,k_in,delta)
    grad_nll_proposed<-grad(theta_proposed,y,a,k_in,delta)
    
    k_bt<-0 # Keeps track of number of backtracks
    # Write out the conditions for modified backtracking
    cond1 <- nll_proposed >= nll_current + control$c1*alpha*crossprod(grad_nll_current,Delta)
    cond2 <- crossprod(grad_nll_proposed,Delta) < control$c2*crossprod(grad_nll_current,Delta)
    
    while(cond1|cond2){ # While either of these conditions holds
      if(cond1){
        alpha_max=alpha
        alpha=(alpha_min+alpha_max)/2
      }else if(cond2){
        alpha_min=alpha
        if(alpha_max==Inf){
          alpha=2*alpha
        }else{
          alpha=(alpha_min+alpha_max)/2
        }
      } 
      # Re-evaluate proposed values for new alpha
      theta_proposed<-theta_current+alpha*Delta
      nll_proposed<-func(theta_proposed,y,a,k_in,delta)
      grad_nll_proposed<-grad(theta_proposed,y,a,k_in,delta)
      # Re-evaluate conditions to go back into loop
      cond1 <- nll_proposed >= nll_current + control$c1*alpha*crossprod(grad_nll_current,Delta)
      cond2 <- crossprod(grad_nll_proposed,Delta) < control$c2*crossprod(grad_nll_current,Delta)
      
      k_bt<-k_bt+1 # Add 1 to number of backtracks
    }
    
    k_backtracking[k]<-k_bt # Store number of backtracks
    # Update and store values
    grad_nll_old<-grad_nll_current
    theta_current<-theta_proposed
    nll_current<-nll_proposed
    grad_nll_current<-grad(theta_current,y,a,k_in,delta)
    # Things we need to update B
    eta<-grad_nll_current-grad_nll_old
    rho <- 1/as.numeric(crossprod(eta,Delta))
    # Update B
    B=(diag(d)-rho*tcrossprod(Delta,eta))%*%B%*%(diag(d)-rho*tcrossprod(eta,Delta))+alpha*rho*tcrossprod(Delta,Delta)
    
    k <- k+1 # Add one to number of iterations
    theta_iterates[k,]<-theta_current # Store the iteration
    
    if (k==control$maxit){
      convergence_indicator<-1
    }
  }
  # Pass back the outputs
  return(list(theta=theta_current,
              nll_value=nll_current,
              convergence=convergence_indicator,
              no_iterations=k,
              iterates=theta_iterates[1:k,],
              inv_hess=B,
              no_backtracks=k_backtracking[1:k]))
}

# Implement the algorithm for our dataset
# Set input arguments from dataset
y = deaths$deaths_COVID
a = deaths$age
delta = deaths$sex_logic
k = deaths$population/100000

# Need to pass an initial matrix B, set this as identity matrix
# There is a default in control so no need to pass
run_BFGS<-BFGS(theta_0=initial_guess,func=nll,grad=nll_grad,B=diag(3),y=y,a=a,k_in=k,delta=delta)
run_BFGS # Display all the information
theta_final<-as.numeric(run_BFGS$theta) # This is the final value of theta from the algorithm
```

```{r}
cat('Number of iterations of this method where', run_BFGS$no_iterations)
```

As you can see Quasi Newton's method took 9 iterations to converge, where as last weeks regular implementation of Newton's method only took 3 iterations to converge. As I reassuring aside are final value for theta agrees across both methods so we can be more confident that we did not make a mistake in are implementation.

## Knit to html

![](https://people.bath.ac.uk/kai21/ASI/img/knit-html-screenshot.png)
