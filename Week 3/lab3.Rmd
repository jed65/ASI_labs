---
title: 'MA40198: Lab sheet 3'
author: "Jake Denton, Patrick Fahy and Henry Writer"
output:
  html_document:
    number_sections: no
    toc: yes
    toc_float: yes
    code_download: yes
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, highlight = TRUE)
library(tidyverse)
```

## Instructions

-   This assignment should be done in groups according to the coursework group allocation.

-   As indicated in each question, you should complete the answers by filling-out the 'Lab3.Rmd' file. You can remove the instructions and introductory sections if you wish.

-   The 'Lab3.Rmd' file can be downloaded by clicking in the **Code download** button at the top right of this page. If the button does not work you can download the 'Lab3.Rmd' file from the Moodle page. You should open this file in RStudio.

-   After completing the answers, each group should convert the 'Lab3.Rmd' file into an HTML file by following the instructions in the <a href="#knit-to-html">Knit to html</a> section at the end of this file!

-   Each group should submit two files: the completed 'Lab3.Rmd' file and the created HTML file (which should be named 'Lab3.html') to the <a href="https://moodle.bath.ac.uk/mod/assign/view.php?id=1164345">submission point in Moodle</a>

-   This is lab is a **formative assessment** and you will receive feedback to your submitted work on an informal basis.

## Introduction

Consider the example in [Section 2.3 of the lecture notes](https://moodle.bath.ac.uk/pluginfile.php/1924827/mod_resource/content/49/docs/01-optimisation.html#sec-example-nbinom-mle) where data on the number of COVID-19 related deaths in 311 local authorities in England is used. A negative Binomial model is assumed for the number of deaths in each local authority. More specifically, we assume $(Y_1,\ldots,Y_{311})$ are independent random variables corresponding to the number of COVID-19 deaths in each local authority and they all follow the same Negative Binomial distribution with unknown size parameter $\eta^*$ and unknown probability parameter $p^*$ (see [Section 2.3.3](https://moodle.bath.ac.uk/pluginfile.php/1924827/mod_resource/content/49/docs/01-optimisation.html#negative-binomial-model)).

The aim of this lab is to understand the asymptotic properties of the loglikelihood function and in particular of the maximimum likelihood estimators.

## Question 1

Generate $N=10,000$ independent samples $\boldsymbol{y}_1,\ldots, \boldsymbol{y}_N$ each of size 311 from a Negative Binomial distribution with size $\eta^*=10$ and probability $p^*=0.05$. You may use the `R` function `rnbinom`.

## Solution to Question 1 {.unnumbered}

```{r Q1, eval = TRUE}
N=10000 #number of samples required
samples<-matrix(0,nrow=N,ncol=311) #matrix to hold samples
for (i in 1:N){ #for loop to generate samples
  samples[i,]<-rnbinom(311,10,0.05) #call rnbinom() to give us samples
} 
```

## Question 2

For each sample $\boldsymbol{y}_i$ compute:

-   the maximum likelihood estimator $\widehat{\boldsymbol{\theta}}(\boldsymbol{y}_i)$. You can use the code in [Section 2.11 of the lecture notes](https://moodle.bath.ac.uk/pluginfile.php/1924827/mod_resource/content/49/docs/01-optimisation.html#example-negative-binomial-maximum-likelihood-2). Note this code uses `R` function `optim` that implements a version of the BFGS quasi-Newton algorithm that you can use.

So we get:

-   a sample $\widehat{\boldsymbol{\theta}}(\boldsymbol{y}_1), \ldots, \widehat{\boldsymbol{\theta}}(\boldsymbol{y}_N)$ of size $N$ from the MLE

-   a sample $\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$ of size $N$ from the gradient evaluated at $(\theta_1^*,\theta_2^*)^T$.

-   a sample $\nabla^2\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\nabla^2\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$ of size $N$ from the Hessian evaluated at $(\theta_1^*,\theta_2^*)^T$.

-   a sample $\Delta(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\Delta(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$ of size $N$ from the Newton descent direction at $(\theta_1^*,\theta_2^*)^T$.

Store this samples for use in the following questions.

## Solution to Question 2 {.unnumbered}

```{r Q2, eval = TRUE}
#Functions given in notes to find negative log-likelihood
loglikelihood_nbinom<-function(size,prob,data){
  
  sum_log_dens <- function(x,size,prob){
    sum(dnbinom(x,size,prob,log = TRUE))  
  }
  
  map2_dbl(.x = size,
           .y = prob,
           .f = sum_log_dens,
           x = data)
  
}
loglikelihood_nbinom3<- function(logsize,logitprob,data){
  
  
  size <- exp(logsize)
  prob  <- exp(logitprob)/(1+exp(logitprob))
  
  loglikelihood_nbinom(size,prob,data)
  
}
negloglik_fn<-function(theta = c(0,0),
                       data = 1){
  
  -loglikelihood_nbinom3(logsize = theta[1],
                         logitprob = theta[2],
                         data    = data)
}
#Custom log-likelihood function in this case, not given in notes
loglik_expr<-expression(lgamma(y+exp(theta1))-lgamma(exp(theta1))-lgamma(y+1)+exp(theta1)*(theta2-log(1+exp(theta2)))-y*log(1+exp(theta2)))
loglik_deriv <- deriv(expr         = loglik_expr,
                         namevec      = c("theta1","theta2"),
                         function.arg = c("theta1","theta2","y"),
                         hessian      = TRUE)
#New function to compute gradient of negative log-likelihood
negloglik_grad <- function(theta = c(0,0),
                           data = 1){
  
  aux  <- loglik_deriv(theta1 = theta[1],
                          theta2 = theta[2],
                          y      = data)
  
  grad <- apply(attr(aux,"gradient"),2,sum)
  
  -grad
}
#New function to compute Hessian
negloglik_hess <- function(theta = c(0,0),
                           data = 1){
  
  aux  <- loglik_deriv(theta1 = theta[1],
                          theta2 = theta[2],
                          y      = data)
  
  hess<-apply(attr(aux,"hessian"),c(2,3),sum)
  
  -hess
}


suppressWarnings({ ## otherwise many warning messages about NaNs appears
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
theta_star<-c(log(10),log(0.05/0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at theta_star
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at theta_star
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at theta_star
for (i in 1:N){
  run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
             method="BFGS",hessian=TRUE) #perform optimisation
  parameter_values[i,]<-run$par #store MLEs
  grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at theta_star
  hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at theta_star
  newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
})
#May get some warnings but all the outputs are sensible
```

## Question 3

Let $[\boldsymbol{a}]_i$ denote the i-th entry of the vector $\boldsymbol{a}$.

With the samples obtained in Question 2, plot the following:

-   a histogram of $[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_1-\theta_1^*$

-   a histogram of $[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_2-\theta_2^*$

-   a scatterplot of$[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_1-\theta_1^*$ vs $[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_2-\theta_2^*$

What can you conclude from these plots?

## Solution to Question 3 {.unnumbered}

```{r Q3, eval = TRUE}
# your code here
theta1_star =  theta_star[1]
theta2_star =  theta_star[2]

# histogram with a normal curve overlayed
x <- sort(grad_values[,1]-theta1_star)
fun <- dnorm(x, mean = theta1_star, sd = sqrt(mean(hessian_values[1,1,])))
hist(x, prob = TRUE)
lines(x, fun, col = 2)


x2 <- sort(grad_values[,2]-theta2_star)
fun <- dnorm(x2, mean = theta2_star, sd = sqrt(mean(hessian_values[2,2,])))
hist(x2, prob = TRUE)
lines(x2, fun, col = 2)



plot(grad_values[,1]-theta1_star, grad_values[,2]-theta2_star)
```


The first two plots show that the distribution of grad_values is approximately normal for both parameters.

The scatter plot shows that the two grad values are very negatively correlated with each other (we get pearson correlation ~ -0.9765 for this sample).


## Question 4

With the samples obtained in Question 2, plot the following:

-   a histogram of $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_1$

-   a histogram of $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_2$

-   a scatter of $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_1$ vs $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_2$

What can you conclude from these plots?

## Solution to Question 4 {.unnumbered}

```{r Q4, eval = TRUE}
# your code here

x <- sort(newton_direction_values[,1])
fun <- dnorm(x, mean = 0, sd = sd(x))
hist(x, prob = TRUE)
lines(x, fun, col = 2)

x2 <- sort(newton_direction_values[,2])
fun <- dnorm(x2, mean = 0, sd = sd(x2))
hist(x2, prob = TRUE)
lines(x2, fun, col = 2)

plot(newton_direction_values[,1], newton_direction_values[,2])
```
The first two plots show that the distribution of values is approximately normal for both parameters.

The scatter plot shows that the two values are very positively correlated with each other (we get pearson correlation ~ 0.9765 for this sample, which is the negative of the correlation in the previous question).

## Question 5

With the samples obtained in Question 2, plot the following:

-   a scatterplot of $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_1$ vs $[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_1-\theta_1^*$

-   a scatterplot of $[\Delta(\boldsymbol{\theta}^*|\boldsymbol{y})]_2$ vs $[\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y})]_2-\theta_2^*$

What can you conclude from these plots?

## Solution to Question 5 {.unnumbered}

```{r Q5, eval = TRUE}
# your code here
plot(newton_direction_values[,1], grad_values[,1]-theta1_star)
plot(newton_direction_values[,2], grad_values[,2]-theta2_star)
```

Seemingly a slight negative correlation for both.

## Question 6

Compute the sample variance-covariance matrix of:

-   $\widehat{\boldsymbol{\theta}}(\boldsymbol{y}_1)-\boldsymbol{\theta}^*, \ldots, \widehat{\boldsymbol{\theta}}(\boldsymbol{y}_N)-\boldsymbol{\theta}^*$, and

-   $\Delta(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\Delta(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$

Also compute the inverse of the sample variance-covariance matrix of $\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\nabla\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$

Finally, compute the inverse of the sample average of $\nabla^2\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_1),\ldots,\nabla^2\ell(\boldsymbol{\theta}^*|\boldsymbol{y}_N)$.

You may use the function `cov` in `R`. What do you notice?

## Solution to Question {.unnumbered}

```{r Q6, eval = TRUE}
#Calculating the first part of the question
theta_star_matrix<-matrix(0,nrow=N,ncol=2)
theta_star_matrix[,1]<-rep(theta_star[1],N)
theta_star_matrix[,2]<-rep(theta_star[2],N)
differences<-parameter_values-theta_star_matrix
varcov_differences<-cov(differences)
var_cov_newton_direction = cov(newton_direction_values)

#Calculating the second part of the question
inv_var_cov_grad = solve(cov(grad_values))

#Calculating the last part of the question
#Creating the average Hessian by computing the mean on each index(i,j,)
avg_hessian = matrix(c(mean(hessian_values[1,1,]),
                        mean(hessian_values[1,2,]), 
                        mean(hessian_values[2,1,]),
                        mean(hessian_values[2,2,])), byrow = TRUE, nrow = 2)
#Inverting it
inv_avg_hessian = solve(avg_hessian)


#Present these 
varcov_differences
inv_avg_hessian

var_cov_newton_direction
inv_var_cov_grad
```
All these matrices are very similar. The variance-covariance matrix of the difference
between the MLEs and known value is closest to the sample average of the inverse Hessian
as we would expect given what we know about the distribution of the asymptotic MLE from the lecture notes (they are equal).
The variance-covariance matrix of the Newton directions is equal to the inverse of the variance-covariance matrix of the gradient values, which is an interesting result.


## Question 7

How can you use the information concluded from the previous questions to construct a confidence interval for the unknown common mean number of COVID-19 deaths $\mu^*$ in the 311 local authorities in England?

## Solution to Question 7 {.unnumbered}

We know from the lecture notes that the MLE has a normal asymptotic distribution with mean $\theta^*$ and variance given by the inverse of the Fisher information matrix (which itself is defined as the expectation of the Hessian with respect to $\theta^*$) which we defined in the previous code chunk as inv_avg_hessian. Using this, we can construct a confidence interval for $\theta^*_1$ and $\theta^*_2$ using the sample averages of the MLEs $(\hat\theta_1, \hat\theta_2)$ and variances from inv_avg_hessian in the standard way. From here, we can use the upper and lower values from these intervals to find the range of values that $\mu=e^{\theta_1-\theta_2}$  can take. This gives the required confidence interval.

## Knit to html {.unnumbered}

![](https://people.bath.ac.uk/kai21/ASI/img/knit-html-screenshot.png)
