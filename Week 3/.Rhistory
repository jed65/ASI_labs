covid_districts
covid_districts<- read_csv(file = "https://people.bath.ac.uk/kai21/ASI/data/local_authority_covid_deaths.csv")
install.packages("tidyverse")
library(tidyverse)
covid_districts<- read_csv(file = "https://people.bath.ac.uk/kai21/ASI/data/local_authority_covid_deaths.csv")
covid_districts
covid_districts<- read_csv(file = "https://people.bath.ac.uk/kai21/ASI/data/local_authority_covid_deaths.csv",header=TRUE)
covid_districts %>% select(local_authority,death_rate) -> covid_districts
covid_districts
#### Question 1 ####
N->10000 #number of samples required
N=10000 #number of samples required
samples<-matrix(0,nrow=N,ncol=311) #matrix to hold samples
for (i in 1:N){ #for loop to generate samples
samples[i,]<-rnbinom(311,10,0.05)
}
samples[1:2,1:5]
N=10000 #number of samples required
samples<-matrix(0,nrow=N,ncol=311) #matrix to hold samples
for (i in 1:N){ #for loop to generate samples
samples[i,]<-rnbinom(311,10,0.05)
}
samples[1:2,1:5]
library(dplyr)
count(samples>311)
which(samples>311)
sum(samples>311)
samples[3,]
rnbinom(5,5,0.5)
rnbinom(5,5,1)
rbinom(5,5,1)
array(0,dim=c(2,2,2))
x=array(0,dim=c(2,2,2))
x[,,1]
loglikelihood_nbinom2<- function(mean,size,data){
sum_log_dens <- function(x,size,mean){
sum(dnbinom(x,size,mu=mean,log = TRUE))
}
map2_dbl(.x = size,
.y = mean,
.f = sum_log_dens,
x = data)
}
loglikelihood_nbinom4<- function(logmean,logsize,data){
mean = exp(logmean)
size = exp(logsize)
loglikelihood_nbinom2(mean,size,data)
}
negloglik_fn<-function(theta = c(0,0), data = 1){
-loglikelihood_nbinom4(logmean = theta[1], logsize = theta[2],data = data)
}
#Next part is for gradient function (also from the notes)
loglik_expr <- expression(lgamma(y+exp(theta2))-lgamma(exp(theta2))-lgamma(y+1) + exp(theta2)*theta2 + y*theta1 -(exp(theta2)+y)*log(exp(theta1)+exp(theta2)) )
negloglik_deriv <- deriv(expr         = loglik_expr,
namevec      = c("theta1","theta2"),
function.arg = c("theta1","theta2","y"),
hessian      = TRUE)
negloglik_grad <- function(theta = c(0,0),
data = 1){
aux  <- negloglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
grad <- apply(attr(aux,"gradient"),2,sum)
-grad
}
#Now we have these, can continue with finding MLEs
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at MLEs
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at MLEs
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(run$par,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-run$hessian #store hessian at MLE
newton_direction_values[i,]<- -solve(run$hessian)%*%t(grad_values[i,]) #calculate newton direction
}
grad_values[1,]
run$hessian
t(grad_values[1,])
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(run$par,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-run$hessian #store hessian at MLE
newton_direction_values[i,]<- -solve(run$hessian)%*%grad_values[i,] #calculate newton direction
}
parameter_values[1:10,]
grad_values[1:10,]
newton_direction_values[1:10,]
hessian_values[,,1:3]
is.nan(parameter_values)
any(is.nan(parameter_values))
any(c(TRUE,FALSE))
any(c(FALSE,FALSE))
any(is.nan(grad_values))
any(is.nan(newton_direction_values))
any(is.nan(hessian_values))
library(tidyverse)
covid_districts<- read_csv(file = "https://people.bath.ac.uk/kai21/ASI/data/local_authority_covid_deaths.csv")
covid_districts %>% select(local_authority,death_rate) -> covid_districts #select only the area and number of deaths
#### Question 1 ####
N=10000 #number of samples required
samples<-matrix(0,nrow=N,ncol=311) #matrix to hold samples
for (i in 1:N){ #for loop to generate samples
samples[i,]<-rnbinom(311,10,0.05) #call rnbinom() to give us samples
}
#### Question 2 ####
#Need the functions from the notes
#First three functions are for negative log-likelihood
loglikelihood_nbinom2<- function(mean,size,data){
sum_log_dens <- function(x,size,mean){
sum(dnbinom(x,size,mu=mean,log = TRUE))
}
map2_dbl(.x = size,
.y = mean,
.f = sum_log_dens,
x = data)
}
loglikelihood_nbinom4<- function(logmean,logsize,data){
mean = exp(logmean)
size = exp(logsize)
loglikelihood_nbinom2(mean,size,data)
}
negloglik_fn<-function(theta = c(0,0), data = 1){
-loglikelihood_nbinom4(logmean = theta[1], logsize = theta[2],data = data)
}
#Next part is for gradient function (also from the notes)
loglik_expr <- expression(lgamma(y+exp(theta2))-lgamma(exp(theta2))-lgamma(y+1) + exp(theta2)*theta2 + y*theta1 -(exp(theta2)+y)*log(exp(theta1)+exp(theta2)) )
negloglik_deriv <- deriv(expr         = loglik_expr,
namevec      = c("theta1","theta2"),
function.arg = c("theta1","theta2","y"),
hessian      = TRUE)
negloglik_grad <- function(theta = c(0,0),
data = 1){
aux  <- negloglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
grad <- apply(attr(aux,"gradient"),2,sum)
-grad
}
#Now we have these, can continue with finding MLEs
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at MLEs
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at MLEs
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(run$par,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-run$hessian #store hessian at MLE
newton_direction_values[i,]<- -solve(run$hessian)%*%grad_values[i,] #calculate newton direction
}
#There is a warning suggesting there could be some NaNs but
#I checked using any(is.nan()) and got false for all 4 storage arrays
hessian_values[0,0,1]
hessian_values[1,1,1]
hessian_values[:,:,1]
hessian_values[,,1]
mean_hessian_top_left=np.mean(hessian_values[1,1,])
mean_hessian_top_left=mean(hessian_values[1,1,])
mean_hessian_top_left
grad_values[1:10,]
sample_average_hessian=matrix(data=c(mean(hessian_values[1,1,]),mean(hessian_values[1,2,]),mean(hessian_values[2,1,]),mean(hessian_values[2,2,])),nrows=2,ncols=2)
sample_average_hessian=matrix(data=c(mean(hessian_values[1,1,]),mean(hessian_values[1,2,]),mean(hessian_values[2,1,]),mean(hessian_values[2,2,])),nrows=2,ncols=2)
mean(hessian_values[1,1,])
mean(hessian_values[1,2,])
mean(hessian_values[2,1,])
mean_hessian_top_left=mean(hessian_values[1,1,])
mean_hessian_top_right=mean(hessian_values[1,2,])
mean_hessian_bot_left=mean(hessian_values[2,1,])
mean_hessian_bot_right=mean(hessian_values[2,2,])
sample_average_hessian=matrix(data=c(mean_hessian_top_left,mean_hessian_top_right,mean_hessian_bot_left,mean_hessian_bot_right),nrows=2,ncols=2)
sample_average_hessian=matrix(c(mean_hessian_top_left,mean_hessian_top_right,mean_hessian_bot_left,mean_hessian_bot_right),nrows=2,ncols=2)
sample_average_hessian=matrix(0,nrows=2,ncols=2)
sample_average_hessian=matrix(0,nrow=2,ncol=2)
sample_average_hessian=matrix(c(mean_hessian_top_left,mean_hessian_top_right,mean_hessian_bot_left,mean_hessian_bot_right),nrow=2,ncol=2)
sample_average_hessian=matrix(c(mean_hessian_top_left,mean_hessian_top_right,mean_hessian_bot_left,mean_hessian_bot_right),nrows=2,ncols=2)
sample_average_hessian=matrix(c(mean_hessian_top_left,mean_hessian_top_right,mean_hessian_bot_left,mean_hessian_bot_right),nrow=2,ncol=2)
sample_average_hessian
inv_sample_average_hessian=solve(sample_average_hessian)
inv_sample_average_hessian
var_grads<-cov(grad_values)
var_grads
0.05/0.95
log(0.05/0.95)
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
#Define the rest for exact theta star
theta_star=c(log(10),log(0.05,0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at exact
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at exact
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at exact
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at MLE
newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
negloglik_hess <- function(theta = c(0,0),
data = 1){
aux <- negloglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y = data)
hess<-apply(attr(aux,"hessian"),c(2,3),sum)
-hess
}
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
#Define the rest for exact theta star
theta_star=c(log(10),log(0.05,0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at exact
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at exact
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at exact
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at MLE
newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
hessian_values
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at MLE
}
for (i in 1:N){
#run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
#method="BFGS",hessian=TRUE) #perform optimisation
#parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at MLE
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at MLE
}
hessian_values
N=10000 #number of samples required
samples<-matrix(0,nrow=N,ncol=311) #matrix to hold samples
for (i in 1:N){ #for loop to generate samples
samples[i,]<-rnbinom(311,size=10,prob=0.05) #call rnbinom() to give us samples
}
loglikelihood_nbinom<-function(size,prob,data){
sum_log_dens <- function(x,size,prob){
sum(dnbinom(x,size,prob,log = TRUE))
}
map2_dbl(.x = size,
.y = prob,
.f = sum_log_dens,
x = data)
}
loglikelihood_nbinom3<- function(logsize,logitprob,data){
size <- exp(logsize)
prob  <- exp(logitprob)/(1+exp(logitprob))
loglikelihood_nbinom(size,prob,data)
}
negloglik_fn<-function(theta = c(0,0),
data = 1){
-loglikelihood_nbinom3(logsize = theta[1],
logitprobsize = theta[2],
data    = data)
}
#Custom log-likelihood function in this case, not given in notes
loglik_expr<-expression(lgamma(y+exp(theta1))-lgamma(exp(theta1))-lgamma(y+1)+exp(theta1)*(theta2-log(1+exp(theta2)))-y*log(1+exp(theta2)))
loglik_deriv <- deriv(expr         = loglik_expr,
namevec      = c("theta1","theta2"),
function.arg = c("theta1","theta2","y"),
hessian      = TRUE)
#New function to compute gradient of negative log-likelihood
negloglik_grad <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
grad <- apply(attr(aux,"gradient"),2,sum)
-grad
}
#New function to compute Hessian
negloglik_hess <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
hess<-apply(attr(aux,"hessian"),c(2,3),sum)
-hess
}
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
theta_star<-c(log(10),log(0.05/0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at theta_star
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at theta_star
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at theta_star
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at theta_star
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at theta_star
newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
loglikelihood_nbinom<-function(size,prob,data){
sum_log_dens <- function(x,size,prob){
sum(dnbinom(x,size,prob,log = TRUE))
}
map2_dbl(.x = size,
.y = prob,
.f = sum_log_dens,
x = data)
}
loglikelihood_nbinom3<- function(logsize,logitprob,data){
size <- exp(logsize)
prob  <- exp(logitprob)/(1+exp(logitprob))
loglikelihood_nbinom(size,prob,data)
}
negloglik_fn<-function(theta = c(0,0),
data = 1){
-loglikelihood_nbinom3(logsize = theta[1],
logitprob = theta[2],
data    = data)
}
#Custom log-likelihood function in this case, not given in notes
loglik_expr<-expression(lgamma(y+exp(theta1))-lgamma(exp(theta1))-lgamma(y+1)+exp(theta1)*(theta2-log(1+exp(theta2)))-y*log(1+exp(theta2)))
loglik_deriv <- deriv(expr         = loglik_expr,
namevec      = c("theta1","theta2"),
function.arg = c("theta1","theta2","y"),
hessian      = TRUE)
#New function to compute gradient of negative log-likelihood
negloglik_grad <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
grad <- apply(attr(aux,"gradient"),2,sum)
-grad
}
#New function to compute Hessian
negloglik_hess <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
hess<-apply(attr(aux,"hessian"),c(2,3),sum)
-hess
}
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
theta_star<-c(log(10),log(0.05/0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at theta_star
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at theta_star
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at theta_star
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at theta_star
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at theta_star
newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
library(tidyverse)
### Question 2 - Correct! ####
loglikelihood_nbinom<-function(size,prob,data){
sum_log_dens <- function(x,size,prob){
sum(dnbinom(x,size,prob,log = TRUE))
}
map2_dbl(.x = size,
.y = prob,
.f = sum_log_dens,
x = data)
}
loglikelihood_nbinom3<- function(logsize,logitprob,data){
size <- exp(logsize)
prob  <- exp(logitprob)/(1+exp(logitprob))
loglikelihood_nbinom(size,prob,data)
}
negloglik_fn<-function(theta = c(0,0),
data = 1){
-loglikelihood_nbinom3(logsize = theta[1],
logitprob = theta[2],
data    = data)
}
#Custom log-likelihood function in this case, not given in notes
loglik_expr<-expression(lgamma(y+exp(theta1))-lgamma(exp(theta1))-lgamma(y+1)+exp(theta1)*(theta2-log(1+exp(theta2)))-y*log(1+exp(theta2)))
loglik_deriv <- deriv(expr         = loglik_expr,
namevec      = c("theta1","theta2"),
function.arg = c("theta1","theta2","y"),
hessian      = TRUE)
#New function to compute gradient of negative log-likelihood
negloglik_grad <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
grad <- apply(attr(aux,"gradient"),2,sum)
-grad
}
#New function to compute Hessian
negloglik_hess <- function(theta = c(0,0),
data = 1){
aux  <- loglik_deriv(theta1 = theta[1],
theta2 = theta[2],
y      = data)
hess<-apply(attr(aux,"hessian"),c(2,3),sum)
-hess
}
parameter_values<-matrix(0,nrow=N,ncol=2) #to store MLEs
theta_star<-c(log(10),log(0.05/0.95))
grad_values<-matrix(0,nrow=N,ncol=2) #to store gradient at theta_star
hessian_values<-array(0,dim=c(2,2,N)) #store Hessian at theta_star
newton_direction_values<-matrix(0,nrow=N,ncol=2) #store Newton descent directions at theta_star
for (i in 1:N){
run<-optim(par=c(0,0),fn=negloglik_fn,gr=negloglik_grad,data=samples[i,],
method="BFGS",hessian=TRUE) #perform optimisation
parameter_values[i,]<-run$par #store MLEs
grad_values[i,]<-negloglik_grad(theta_star,samples[i,]) #store gradient at theta_star
hessian_values[,,i]<-negloglik_hess(theta_star,samples[i,]) #store hessian at theta_star
newton_direction_values[i,]<- -solve(hessian_values[,,i])%*%grad_values[i,] #calculate newton direction
}
parameter_values[1,]
exp(parameter_values[1,])
exp(parameter_values[1:5,])
grad_values[1,]
grad_values[2,]
grad_values[1:10,]
mean(grad_values)
mean(grad_values[1:N,])
mean(grad_values[,1])
mean(grad_values[,2])
hessian_values[,,1]
hessian_values[,,2]
hessian_values[,,3]
newton_direction_values[1,]
newton_direction_values[2,]
sample_varcov<-cov(grad_values)
sample_varcov
sample_avg_hess<-matrix(0,nrow=2,ncol=2)
sample_avg_hess[1,1]<-mean(hessian_values[1,1,])
sample_avg_hess[1,1]
sample_avg_hess[1,2]<-mean(hessian_values[1,2,])
sample_avg_hess[2,1]<-mean(hessian_values[2,1,])
sample_avg_hess[2,2]<-mean(hessian_values[2,2,])
sample_varcov
sample_avg_hess
inv_grad_varcov<-solve(sample_varcov)
inv_avg_hess<-solve(sample_avg_hess)
inv_grad_varcov
inv_avg_hess
theta_star_matrix<-matrix(0,nrow=N,ncol=2)
theta_star_matrix[,1]<-rep(theta_star[1],N)
theta_star_matrix[,2]<-rep(theta_star[2],N)
theta_star_matrix[1:5,]
differences<-parameter_values-theta_star_matrix
differences[1:5,]
varcov_differences<-cov(differences)
varcov_differences
varcov_newton_direction<-cov(newton_direction_values)
varcov_newton_direction
varcov_differences
varcov_newton_direction
inv_grad_varcov
inv_avg_hess
knitr::opts_chunk$set(echo = TRUE, highlight = TRUE)
library(tidyverse)
theta2_star =  log(10)
theta1_star =  log(10*0.95/0.05)
hist(grad_values[,1]-theta1_star)
hist(grad_values[,2]-theta2_star)
plot(grad_values[,1]-theta1_star, grad_values[,2]-theta2_star)
hist(newton_direction_values[,1])
hist(newton_direction_values[,2])
plot(newton_direction_values[,1], newton_direction_values[,2])
plot(newton_direction_values[,1], grad_values[,1]-theta1_star)
plot(newton_direction_values[,2], grad_values[,2]-theta2_star)
theta_star_matrix<-matrix(0,nrow=N,ncol=2)
theta_star_matrix[,1]<-rep(theta_star[1],N)
theta_star_matrix[,2]<-rep(theta_star[2],N)
differences<-parameter_values-theta_star_matrix
varcov_differences<-cov(differences)
var_cov_newton_direction = cov(newton_direction_values)
#Calculating the second part of the question
inv_var_cov_grad = solve(cov(grad_values))
#Calculating the last part of the question
#Creating the average Hessian by computing the mean on each index(i,j,)
avg_hessian = matrix(c(mean(hessian_values[1,1,]),
mean(hessian_values[1,2,]),
mean(hessian_values[2,1,]),
mean(hessian_values[2,2,])), byrow = TRUE, nrow = 2)
#Inverting it
inv_avg_hessian = solve(avg_hessian)
#Present these
varcov_differences
inv_avg_hessian
var_cov_newton_direction
inv_var_cov_grad
